name: Phase 11 Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # Job 1: Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-mock
    
    - name: Run unit tests
      env:
        TESTING: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
      run: |
        cd services/rag_api
        python -m pytest tests/unit/ -v --tb=short --durations=10 --junitxml=test-results-unit.xml
    
    - name: Upload unit test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results
        path: services/rag_api/test-results-unit.xml

  # Job 2: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25
    
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
        options: >-
          --health-cmd "curl -f http://localhost:6333/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-mock
    
    - name: Wait for Qdrant to be ready
      run: |
        for i in {1..30}; do
          if curl -f http://localhost:6333/health; then
            echo "Qdrant is ready"
            break
          fi
          echo "Waiting for Qdrant... attempt $i"
          sleep 2
        done
    
    - name: Run integration tests
      env:
        TESTING: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
        QDRANT_URL: http://localhost:6333
      run: |
        cd services/rag_api
        python -m pytest tests/integration/ -v --tb=short --durations=10 --junitxml=test-results-integration.xml
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: services/rag_api/test-results-integration.xml

  # Job 3: Edge Case Tests
  edge-case-tests:
    name: Edge Case Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-mock
    
    - name: Run edge case tests
      env:
        TESTING: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
      run: |
        cd services/rag_api
        python -m pytest tests/edge_cases/ -v --tb=short --durations=10 --junitxml=test-results-edge-cases.xml
    
    - name: Upload edge case test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: edge-case-test-results
        path: services/rag_api/test-results-edge-cases.xml

  # Job 4: Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-mock psutil
    
    - name: Run performance tests
      env:
        TESTING: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
      run: |
        cd services/rag_api
        python -m pytest tests/performance/ -v --tb=short --durations=10 --junitxml=test-results-performance.xml -m "not slow"
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: services/rag_api/test-results-performance.xml

  # Job 5: Code Quality and Coverage
  code-quality:
    name: Code Quality & Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio pytest-mock ruff black isort
    
    - name: Run linting (ruff)
      run: |
        cd services/rag_api
        ruff check . --output-format=github
    
    - name: Check code formatting (black)
      run: |
        cd services/rag_api
        black --check --diff .
    
    - name: Check import sorting (isort)
      run: |
        cd services/rag_api
        isort --check-only --diff .
    
    - name: Run tests with coverage
      env:
        TESTING: true
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
      run: |
        cd services/rag_api
        python -m pytest tests/ --cov=./ --cov-report=xml --cov-report=html --cov-fail-under=70 --junitxml=test-results-coverage.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: services/rag_api/coverage.xml
        flags: rag-api
        name: rag-api-coverage
    
    - name: Upload coverage results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: coverage-report
        path: |
          services/rag_api/coverage.xml
          services/rag_api/htmlcov/

  # Job 6: Frontend Tests
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: apps/web/package-lock.json
    
    - name: Install dependencies
      run: |
        cd apps/web
        npm ci
    
    - name: Run linting
      run: |
        cd apps/web
        npm run lint
    
    - name: Run frontend tests (if they exist)
      run: |
        cd apps/web
        npm test -- --watchAll=false --coverage --passWithNoTests
    
    - name: Upload frontend test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-test-results
        path: apps/web/coverage/

  # Job 7: End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'push' || github.event_name == 'schedule'
    
    services:
      qdrant:
        image: qdrant/qdrant:latest
        ports:
          - 6333:6333
        options: >-
          --health-cmd "curl -f http://localhost:6333/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: apps/web/package-lock.json
    
    - name: Install Python dependencies
      run: |
        cd services/rag_api
        pip install -r requirements.txt
    
    - name: Install Node.js dependencies
      run: |
        cd apps/web
        npm ci
    
    - name: Wait for Qdrant
      run: |
        for i in {1..30}; do
          if curl -f http://localhost:6333/health; then
            echo "Qdrant is ready"
            break
          fi
          echo "Waiting for Qdrant... attempt $i"
          sleep 2
        done
    
    - name: Start RAG API
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test_key_placeholder' }}
        OPENAI_MODEL: gpt-4o-mini
        EMBEDDING_MODEL: text-embedding-3-small
        QDRANT_URL: http://localhost:6333
      run: |
        cd services/rag_api
        uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: Start Frontend
      run: |
        cd apps/web
        npm run build
        npm start &
        sleep 10
    
    - name: Run E2E tests
      run: |
        cd services/rag_api
        python -m pytest tests/integration/test_rag_workflows.py::TestRAGWorkflowComparison -v --tb=short
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: services/rag_api/test-results-e2e.xml

  # Job 8: Test Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, edge-case-tests, performance-tests, code-quality, frontend-tests]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# Phase 11 Test Suite Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check each job status
        if [ "${{ needs.unit-tests.result }}" == "success" ]; then
          echo "âœ… Unit Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Unit Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.integration-tests.result }}" == "success" ]; then
          echo "âœ… Integration Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Integration Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.edge-case-tests.result }}" == "success" ]; then
          echo "âœ… Edge Case Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Edge Case Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.performance-tests.result }}" == "success" ]; then
          echo "âœ… Performance Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Performance Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.code-quality.result }}" == "success" ]; then
          echo "âœ… Code Quality & Coverage: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Code Quality & Coverage: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${{ needs.frontend-tests.result }}" == "success" ]; then
          echo "âœ… Frontend Tests: PASSED" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ Frontend Tests: FAILED" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Test Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Unit test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Integration test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Coverage report available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Generated on: $(date)" >> $GITHUB_STEP_SUMMARY
    
    - name: Set overall status
      run: |
        if [ "${{ needs.unit-tests.result }}" == "success" ] && \
           [ "${{ needs.integration-tests.result }}" == "success" ] && \
           [ "${{ needs.edge-case-tests.result }}" == "success" ] && \
           [ "${{ needs.performance-tests.result }}" == "success" ] && \
           [ "${{ needs.code-quality.result }}" == "success" ] && \
           [ "${{ needs.frontend-tests.result }}" == "success" ]; then
          echo "ðŸŽ‰ ALL TESTS PASSED! Phase 11 test suite complete."
          exit 0
        else
          echo "ðŸ’¥ SOME TESTS FAILED! Please review the failures above."
          exit 1
        fi
